"experiment",
"d",
"repetitions",
"m",
"n",
"approx_type",
"kernel_type",
"l_minus_l_plus",
"function_type",
"perturbation_or_Qi",
)
index = pd.MultiIndex.from_tuples(index_vals, names=index_names)
results_df = pd.Series(results, index=index).to_frame("power")
results_df.reset_index().to_csv("user/raw/results.csv")
"""
Running this script creates two files results.csv and results.plk in
the 'user/raw' directory containing the relevant parameters and the estimated
power/level of the tests. We use the following numbering of experiments:
uniform mnist
1      2
3      4
5      6
7      8
9      10
11     12
Experiments i and i+1 are the same but using the uniform and mnist data.
We first run the uniform experiments followed by the mnist experiments.
The settings of all those experiments can be understood by their
relations to the figures presented in our paper:
Figure 3 and 4: experiments 1 and 3
Figure 5: experiments 2 and 4
Figure 6: experiments 9 and 10
Figure 7: experiments 1, 2, 3 and 4
Figure 8: experiments 7 and 8
Figure 9: experiments 1, 2, 11 and 12
Table 1: experiments 5 and 6
The figures and table can be created by running the figures.py script.
"""
import numpy as np
import itertools
import pandas as pd
from mnist import download_mnist, load_mnist
from pathlib import Path
from seed import generate_seed
from sample_test import sample_and_test_uniform, sample_and_test_mnist
import argparse
# create results directory if it does not exist
Path("user/raw").mkdir(exist_ok=True, parents=True)
# panda dataframe: lists of indices and entries
index_vals = []
results = []
# parameters shared for all experiments
alpha = 0.05
B1 = 500
B2 = 500
B3 = 100
kernel_types = ["gaussian", "laplace"]
k_num = len(kernel_types)
approx_types = ["wild bootstrap"]
a_num = len(approx_types)
############# UNIFORM #############
# parameters for all uniform experiments
s = 1
perturbation_multipliers = [2.7]
bandwidth_multipliers = np.linspace(0.1, 1, 10)
e_num = 1
p_num = 6
# Experiment 1
exp = "1: uniform alternative"
repetitions = 500
sample_sizes = [500]
L = [(-6, -2)]
l_num = len(L)
function_types = ["increasing"]
f_num = len(function_types)
for a, k, e, l, f, p in itertools.product(
range(a_num), range(k_num), range(e_num), range(l_num), range(f_num), range(p_num)
):
print (k,p)
if (a, l) not in [(1, 0), (1, 2)] and (e, p) != (1, 3):
approx_type = approx_types[a]
kernel_type = kernel_types[k]
d = e + 1
n = m = sample_sizes[e]
perturbation_multiplier = perturbation_multipliers[e]
l_minus, l_plus = L[l]
l_minus_l_plus = L[l]
function_type = function_types[f]
perturbation = p + 1
perturbation_or_Qi = perturbation
test_output_list = []
for i in range(repetitions):
seed = generate_seed(k, e, l, f, p, i)
test_output_list.append(
sample_and_test_uniform(
function_type,
seed,
kernel_type,
approx_type,
m,
n,
d,
perturbation,
s,
perturbation_multiplier,
alpha,
l_minus,
l_plus,
B1,
B2,
B3,
bandwidth_multipliers,
)
)
power = np.mean(test_output_list)
index_val = (
exp,
d,
repetitions,
m,
n,
approx_type,
kernel_type,
l_minus_l_plus,
function_type,
perturbation_or_Qi,
)
index_vals.append(index_val)
results.append(power)
print('Experiment 1 completed.')
# save panda dataframe
index_names = (
"experiment",
"d",
"repetitions",
"m",
"n",
"approx_type",
"kernel_type",
"l_minus_l_plus",
"function_type",
"perturbation_or_Qi",
)
index = pd.MultiIndex.from_tuples(index_vals, names=index_names)
results_df = pd.Series(results, index=index).to_frame("power")
results_df.reset_index().to_csv("user/raw/results.csv")
"""
Running this script creates two files results.csv and results.plk in
the 'user/raw' directory containing the relevant parameters and the estimated
power/level of the tests. We use the following numbering of experiments:
uniform mnist
1      2
3      4
5      6
7      8
9      10
11     12
Experiments i and i+1 are the same but using the uniform and mnist data.
We first run the uniform experiments followed by the mnist experiments.
The settings of all those experiments can be understood by their
relations to the figures presented in our paper:
Figure 3 and 4: experiments 1 and 3
Figure 5: experiments 2 and 4
Figure 6: experiments 9 and 10
Figure 7: experiments 1, 2, 3 and 4
Figure 8: experiments 7 and 8
Figure 9: experiments 1, 2, 11 and 12
Table 1: experiments 5 and 6
The figures and table can be created by running the figures.py script.
"""
import numpy as np
import itertools
import pandas as pd
from mnist import download_mnist, load_mnist
from pathlib import Path
from seed import generate_seed
from sample_test import sample_and_test_uniform, sample_and_test_mnist
import argparse
# create results directory if it does not exist
Path("user/raw").mkdir(exist_ok=True, parents=True)
# panda dataframe: lists of indices and entries
index_vals = []
results = []
# parameters shared for all experiments
alpha = 0.05
B1 = 500
B2 = 500
B3 = 100
kernel_types = ["gaussian", "laplace"]
k_num = len(kernel_types)
approx_types = ["wild bootstrap"]
a_num = len(approx_types)
############# UNIFORM #############
# parameters for all uniform experiments
s = 1
perturbation_multipliers = [2.7]
bandwidth_multipliers = np.linspace(0.1, 1, 10)
e_num = 1
p_num = 6
# Experiment 1
exp = "1: uniform alternative"
repetitions = 500
sample_sizes = [500]
L = [(-6, -2)]
l_num = len(L)
function_types = ["increasing"]
f_num = len(function_types)
for a, k, e, l, f, p in itertools.product(
range(a_num), range(k_num), range(e_num), range(l_num), range(f_num), range(p_num)
):
print (k,p)
if (a, l) not in [(1, 0), (1, 2)] and (e, p) != (1, 3):
approx_type = approx_types[a]
kernel_type = kernel_types[k]
d = e + 1
n = m = sample_sizes[e]
perturbation_multiplier = perturbation_multipliers[e]
l_minus, l_plus = L[l]
l_minus_l_plus = L[l]
function_type = function_types[f]
perturbation = p + 1
perturbation_or_Qi = perturbation
test_output_list = []
for i in range(repetitions):
seed = generate_seed(k, e, l, f, p, i)
test_output_list.append(
sample_and_test_uniform(
function_type,
seed,
kernel_type,
approx_type,
m,
n,
d,
perturbation,
s,
perturbation_multiplier,
alpha,
l_minus,
l_plus,
B1,
B2,
B3,
bandwidth_multipliers,
)
)
power = np.mean(test_output_list)
index_val = (
exp,
d,
repetitions,
m,
n,
approx_type,
kernel_type,
l_minus_l_plus,
function_type,
perturbation_or_Qi,
)
index_vals.append(index_val)
results.append(power)
print('Experiment 1 completed.')
# save panda dataframe
index_names = (
"experiment",
"d",
"repetitions",
"m",
"n",
"approx_type",
"kernel_type",
"l_minus_l_plus",
"function_type",
"perturbation_or_Qi",
)
index = pd.MultiIndex.from_tuples(index_vals, names=index_names)
results_df = pd.Series(results, index=index).to_frame("power")
results_df.reset_index().to_csv("user/raw/results.csv")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/MMDAgg Comparisons/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
reticulate::source_python('~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/MMDAgg Comparisons/MMDAgg-Paper/seed.py')
reticulate::repl_python()
setwd("~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/MMDAgg Comparisons/MMDAgg-Paper")
reticulate::repl_python()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/MMDAgg Comparisons/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
index_vals.r <- py$index_vals
results.r <- py$results
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/MMDAgg Comparisons/MMDAgg-Paper')
reticulate::repl_python()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/MMDAgg Comparisons/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
index_vals.r <- py$index_vals
results.r <- py$results
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
index_vals.r <- py$index_vals
results.r <- py$results
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/MMDAgg Comparisons/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
index_vals.r <- py$index_vals
results.r <- py$results
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/MMDAgg Comparisons/DIM 1/MMDAgg Test/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
index_vals.r <- py$index_vals
results.r <- py$results
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/Revision Simulations/Revision II Simulations/Oracle Split MMD/Oracle Tests/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/Revision Simulations/Revision II Simulations/Oracle Split MMD/Oracle Tests/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
index_vals.r <- py$index_vals
results.r <- py$results
seq(0.1, 2, length.out = 10)
seq(0.1, 2, length.out = 11)
seq(0.1, 1, length.out = 11)
seq(0.1, 1, length.out = 10)
seq(0.1, 2, length.out = 20)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/Revision Simulations/Revision II Simulations/Oracle Split MMD/Oracle Tests/MMDAgg-Paper')
library(reticulate)
reticulate::conda_install(
packages = c("cvxopt", "scikit-learn"),
)
reticulate::conda_install(packages = "numba")
reticulate::py_install(
packages = c("numpy", "numba", "pandas", "scipy", "torch",
"dataframe_image", "matplotlib"),
pip = TRUE
)
reticulate::repl_python()
index_vals.r <- py$index_vals
results.r <- py$results
results.r
setwd("~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/Revision Simulations/Revision II Simulations/Oracle Split MMD/Results")
GEXP<-read.csv("MultiPower-GEXP.csv")
LAPMultiple<-read.csv("MultiPower-LAP.csv")
Mixed<-read.csv("MultiPower-MIXED.csv")
FR<-read.csv("Power-FR.csv")
d=GEXP[,2]
library(tidyverse)
MMDAgg.results <- read_csv("results.csv")
View(MMDAgg.results)
MMDAgg.gaussian.power <- MMDAgg.results %>%
filter(experiment == "1: uniform alternative") %>%
filter(kernel_type == "gaussian") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
View(MMDAgg.gaussian.power)
MMDAgg.gaussian.power <- MMDAgg.results %>%
filter(experiment == "3: uniform alternative") %>%
filter(kernel_type == "gaussian") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
View(MMDAgg.results)
View(MMDAgg.gaussian.power)
MMDAgg.laplace.power <- MMDAgg.results %>%
filter(experiment == "3: uniform alternative") %>%
filter(kernel_type == "laplace") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
MMDAgg.split.gauss.power <- MMDAgg.results %>%
filter(experiment == "3: uniform alternative") %>%
filter(kernel_type == "gaussian") %>%
filter(function_type == "split") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
MMDAgg.oracle.gauss.power <- MMDAgg.results %>%
filter(experiment == "3: uniform alternative") %>%
filter(kernel_type == "gaussian") %>%
filter(function_type == "split (doubled sample sizes)") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
GEXP<-read.csv("MultiPower-GEXP.csv")
LAPMultiple<-read.csv("MultiPower-LAP.csv")
Mixed<-read.csv("MultiPower-MIXED.csv")
FR<-read.csv("Power-FR.csv")
#GAUSSSingle<-read.csv("SinglePower-GAUSS.csv")
#LAPLACESingle<-read.csv("SinglePower-LAP.csv")
d=GEXP[,2]
library(tidyverse)
results <- read_csv("results.csv")
split.gauss.power <- results %>%
filter(experiment == "3: uniform alternative") %>%
filter(kernel_type == "gaussian") %>%
filter(function_type == "split") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
oracle.gauss.power <- results %>%
filter(experiment == "3: uniform alternative") %>%
filter(kernel_type == "gaussian") %>%
filter(function_type == "split (doubled sample sizes)") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
split.lap.power <- results %>%
filter(experiment == "3: uniform alternative") %>%
filter(kernel_type == "laplace") %>%
filter(function_type == "split") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
oracle.lap.power <- results %>%
filter(experiment == "3: uniform alternative") %>%
filter(kernel_type == "laplace") %>%
filter(function_type == "split (doubled sample sizes)") %>%
filter(d == 1) %>%
select(c(perturbation_or_Qi,power)) %>%
as.matrix()
pdf(file="SplitOracleCompareDim1.pdf")
plot(d, GEXP[,3], type='b', col=1, pch=1, lwd=1.5,
ylim=c(0, 1), xlab="Perturbations",
ylab='Power', main="Perturbed One Dimensional Uniform Distribution")
points(d, Mixed[,3], type='b', col=3, pch=3, lwd=1.5)
points(d, LAPMultiple[,3], type='b', col=2, pch=2, lwd=1.5)
points(d, FR[,3], type='b', col=4, pch=4, lwd=1.5)
dev.off()
source("~/Dropbox (Penn)/Kernel-Two-Sample/Simulations/Revision Simulations/Revision II Simulations/Oracle Split MMD/Results/Figure.R", echo=TRUE)
